{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "#记录程序运行时间\n",
    "import time \n",
    "start_time = time.time()\n",
    "\n",
    "#读入数据\n",
    "train = pd.read_csv(r'D:\\pythonPro\\digit\\data\\train.csv')\n",
    "tests = pd.read_csv(r'D:\\pythonPro\\digit\\data\\test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用sklearn.cross_validation进行训练数据集划分，这里训练集和交叉验证集比例为7：3，可以自己根据需要设置\n",
    "train_xy,val = train_test_split(train, test_size = 0.3,random_state=1)\n",
    "\n",
    "y = train_xy.label\n",
    "X = train_xy.drop(['label'],axis=1)\n",
    "val_y = val.label\n",
    "val_X = val.drop(['label'],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, X, Y,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X, label=Y)\n",
    "        cvresult = xgb.cv(xgb_param, #Booster params.\n",
    "                          xgtrain, \n",
    "                          num_boost_round=alg.get_params()['n_estimators'], #迭代次数\n",
    "                          nfold=cv_folds,   #交叉验证中折叠的次数\n",
    "                          metrics='auc',    #评估指标\n",
    "                          early_stopping_rounds=early_stopping_rounds, #需要在每一轮Early_stopping_rounds中至少改善一次以继续训练\n",
    "                          verbose_eval=False  #是否显示进度。 如果为None，则返回np.ndarray时将显示进度。 如果为True，则进度将在增强阶段显示。 如果给定一个整数，则将在每个给定的verbose_eval提升阶段显示进度。\n",
    "                          )\n",
    "        print(cvresult)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X, Y,eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(X)\n",
    "    dtrain_predprob = alg.predict_proba(X)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"Model Report\")\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(Y.values, dtrain_predictions))\n",
    "    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(Y, dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XGBClassifier' object has no attribute 'DMatrix'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-2d4daa0ddf8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m  \u001b[0mscale_pos_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m#在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m  seed=27)\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodelfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-add63dd24b1a>\u001b[0m in \u001b[0;36mmodelfit\u001b[1;34m(alg, X, Y, useTrainCV, cv_folds, early_stopping_rounds)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0museTrainCV\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mxgb_param\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_xgb_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mxgtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         cvresult = xgb.cv(xgb_param, #Booster params.\n\u001b[0;32m      7\u001b[0m                           \u001b[0mxgtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'XGBClassifier' object has no attribute 'DMatrix'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(\n",
    " learning_rate =0.1,  #学习率\n",
    " n_estimators=1000,   #基分类器数量\n",
    " max_depth=5,         #最大深度\n",
    " min_child_weight=1,  #最小叶子节点样本权重之和\n",
    " gamma=0,             #Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守\n",
    " subsample=0.8,       #这个参数控制对于每棵树，随机采样的比例。 减小这个参数的值，算法会更加保守，避免过拟合。\n",
    " colsample_bytree=0.8,#和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。\n",
    " objective= 'multi:softmax', #这个参数定义需要被最小化的损失函数 \n",
    " num_class=10,      # 类别数，与 multisoftmax 并用\n",
    " n_jobs=4,           #用于运行xgboost的并行线程数。\n",
    " scale_pos_weight=0,  #在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。\n",
    " seed=27)\n",
    "modelfit(xgb1, train.drop(['label'],axis=1), train.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n0    4132\n1    4684\n2    4177\n3    4351\n4    4072\n5    3795\n6    4137\n7    4401\n8    4063\n9    4188\nName: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
